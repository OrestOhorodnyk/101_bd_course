find / -name "hadoop-mapreduce-examples*.jar" -print
/usr/hdp/3.0.1.0-187/hadoop-mapreduce/hadoop-mapreduce-examples.jar
/usr/hdp/3.0.1.0-187/hadoop-mapreduce/hadoop-mapreduce-examples-3.1.1.3.0.1.0-187.jar
export HADOOP_EXAMPLES=/usr/hdp/3.0.1.0-187/hadoop-mapreduce
yarn jar $HADOOP_EXAMPLES/hadoop-mapreduce-examples.jar
yarn jar $HADOOP_EXAMPLES/hadoop-mapreduce-examples-3.1.1.3.0.1.0-187.jar


http://resharp.minsk.epam.com/

/usr/hdp/3.0.1.0-187/hadoop-mapreduce/hadoop-streaming-3.1.1.3.0.1.0-187.jar

hadoop fs -mkdir -p /usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft

hadoop fs -put 000000 /usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft


hdfs dfs -rm -r hdfs://sandbox-hdp.hortonworks.com:8020/usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft-output




Let's run the Python code on the Ulysses.txt file.
We'll assume that the Python code is stored in ~hadoop/352/dft/python
We'll assume that the streaming java library is in ~hadoop/contrib/streaming/streaming-0.19.2-streaming.jar
We'll also assume that ulysses.txt is in dft and that we want the output in dft-output:
cd
cd 352/dft/python
hadoop dfs -rmr dft1-output 
hadoop jar /usr/hdp/3.0.1.0-187/hadoop-mapreduce/hadoop-streaming-3.1.1.3.0.1.0-187.jar -file ./mapper.py \
        -mapper ./mapper.py -file ./reducer.py -reducer ./reducer.py  -input /usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft -output /usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft-output


hadoop jar /usr/hdp/3.0.1.0-187/hadoop-mapreduce/hadoop-streaming-3.1.1.3.0.1.0-187.jar -jobconf mapred.reduce.tasks=16  -file ./mapper.py \
        -mapper ./mapper.py -file ./reducer.py -reducer ./reducer.py  -input /usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft -output /usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft-output


        -D mapred.output.compress=true -D mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec

org.apache.hadoop.io.compress.SnappyCodec

# compress with Snappy
hadoop jar /usr/hdp/3.0.1.0-187/hadoop-mapreduce/hadoop-streaming-3.1.1.3.0.1.0-187.jar -D mapred.reduce.tasks=16 -D mapred.output.compress=true -D mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec -file ./mapper.py  -mapper ./mapper.py -file ./reducer.py -reducer ./reducer.py -input /usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft -output /usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft-output

# compress with Snappy
hadoop jar /usr/hdp/3.0.1.0-187/hadoop-mapreduce/hadoop-streaming-3.1.1.3.0.1.0-187.jar -D mapred.reduce.tasks=16 -D mapred.output.compress=true -D mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec -file ./mapper.py  -mapper ./mapper.py -file ./reducer.py -reducer ./reducer.py -input /usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft -output /usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft-output


#compress with Snappy one reduce task
hadoop jar /usr/hdp/3.0.1.0-187/hadoop-mapreduce/hadoop-streaming-3.1.1.3.0.1.0-187.jar -D mapred.output.compress=true -D mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec -file ./mapper.py  -mapper ./mapper.py -file ./reducer.py -reducer ./reducer.py -input /usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft -output /usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft-output


# read snappy file
hadoop fs -text hdfs://sandbox-hdp.hortonworks.com:8020/usr/hdp/3.0.1.0-187/hadoop-mapreduce/word_count/dft-output/part-00000.snappy | head -10
